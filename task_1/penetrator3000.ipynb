{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public samples: 20000\n",
      "Private samples: 20000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "\n",
    "        self.ids = []\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int]:\n",
    "        id_ = self.ids[index]\n",
    "        img = self.imgs[index]\n",
    "        if not self.transform is None:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[index]\n",
    "        return id_, img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "class MembershipDataset(TaskDataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super().__init__(transform)\n",
    "        self.membership = []\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int, int]:\n",
    "        id_, img, label = super().__getitem__(index)\n",
    "        return id_, img, label\n",
    "\n",
    "# Load public dataset\n",
    "# List all files in the current directory\n",
    "# print(\"Files in directory:\", os.listdir(\"./task_1/\"))\n",
    "public_data = torch.load(\"task_1/pub.pt\", map_location=device, weights_only=False)\n",
    "private_data = torch.load(\"task_1/priv_out.pt\", map_location=device, weights_only=False)\n",
    "\n",
    "# Load model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 44)\n",
    "model.load_state_dict(torch.load(\"task_1/01_MIA_69.pt\",  weights_only=False))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"Public samples:\", len(public_data))\n",
    "print(\"Private samples:\", len(private_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_scores(model, dataloader):\n",
    "    model.eval()\n",
    "    confidences = []\n",
    "    losses = []\n",
    "    entropy_vals = []\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ids, images, labels in tqdm(dataloader):  # Now expecting 4 elements\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            pred_classes = probs.argmax(dim=1)\n",
    "\n",
    "            # Get the confidence of the predicted class\n",
    "            confidence = probs[torch.arange(len(labels)), pred_classes]\n",
    "            confidences.extend(confidence.cpu().numpy())\n",
    "\n",
    "            # Compute per-sample loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.extend(loss.cpu().numpy())\n",
    "\n",
    "            # Compute entropy (uncertainty measure)\n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)\n",
    "            entropy_vals.extend(entropy.cpu().numpy())\n",
    "\n",
    "    return np.array(confidences), np.array(losses), np.array(entropy_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 79/79 [00:02<00:00, 37.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   confidence       loss   entropy  membership\n",
      "0    0.989788   7.835775  0.066668          18\n",
      "1    0.999324  11.749366  0.006419          18\n",
      "2    0.684836   5.230702  0.697159          11\n",
      "3    0.998052  10.163871  0.016427          18\n",
      "4    0.408767  11.488248  1.661287           1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "# mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "\n",
    "# Convert public dataset to DataLoader\n",
    "public_loader = DataLoader(public_data, batch_size=256, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# Get confidence scores, losses, and entropy\n",
    "confidences, losses, entropies = get_confidence_scores(model, public_loader)\n",
    "\n",
    "# Extract membership labels\n",
    "membership_labels = np.array([sample[2] for sample in public_data])\n",
    "\n",
    "# Create DataFrame\n",
    "attack_train_df = pd.DataFrame({\n",
    "    \"confidence\": confidences,\n",
    "    \"loss\": losses,\n",
    "    \"entropy\": entropies,\n",
    "    \"membership\": membership_labels\n",
    "})\n",
    "\n",
    "print(attack_train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: /net/tscratch/people/tutorial004/ai\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.6539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Split into train & validation sets\n",
    "X = attack_train_df[[\"confidence\", \"loss\", \"entropy\"]]\n",
    "y = attack_train_df[\"membership\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Train logistic regression model\n",
    "attack_model = LogisticRegression(max_iter=1000)\n",
    "attack_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate model\n",
    "y_pred = attack_model.predict_proba(X_val)  # Keep full probability matrix\n",
    "y_train = (y_train > 0).astype(int)  # Convert to 0/1\n",
    "y_val = (y_val > 0).astype(int)      # Convert to 0/1\n",
    "\n",
    "\n",
    "auc_score = roc_auc_score(y_val, y_pred[:, 1], multi_class=\"ovr\")\n",
    "print(f\"Validation AUC: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 62.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convert private dataset to DataLoader\n",
    "private_loader = DataLoader(private_data, batch_size=256, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# Compute features for private dataset\n",
    "confidences, losses, entropies = get_confidence_scores(model, private_loader)\n",
    "\n",
    "# Create DataFrame\n",
    "attack_test_df = pd.DataFrame({\n",
    "    \"confidence\": confidences,\n",
    "    \"loss\": losses,\n",
    "    \"entropy\": entropies\n",
    "})\n",
    "\n",
    "# Predict membership probability\n",
    "attack_test_df[\"membership_score\"] = attack_model.predict_proba(attack_test_df)[:, 1]\n",
    "\n",
    "# Save submission\n",
    "attack_test_df[[\"membership_score\"]].to_csv(\"submission_test.csv\", index=False)\n",
    "\n",
    "print(\"Submission file saved as 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_test_df['score'] = attack_test_df['membership_score']\n",
    "\n",
    "attack_test_df['ids'] = private_data.ids\n",
    "\n",
    "attack_test_df_csv = attack_test_df[['ids', 'score']]\n",
    "\n",
    "attack_test_df_csv.to_csv('submission_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ids,score\\n251500,2.0737709737836e-06\\n188458,0.0011832338177035857\\n197856,0.0006052600082675227\\n195124,0.005617410652873563\\n244309,0.0047739615261675385\\n156489,3.244530218780111e-12\\n301921,0.0007838206308935009\\n104556,4.449307132976715e-05\\n51198,4.88517458745045e-07\\n18228,0.000796918920309298\\n67500,0.23487232314509351\\n40882,0.015589113263112788\\n141134,9.391125995547876e-08\\n165428,0.2875338813683129\\n121874,8.920330646459018e-08\\n111598,3.341836213271293e-10\\n245332,0.6285293663222967\\n155741,7.80467743660879e-13\\n293665,0.0008104677153106853\\n113568,4.8829541530267894e-08\\n116817,9.33740151473922e-06\\n222038,4.11093595625908e-07\\n94598,7.776056924544796e-08\\n248501,0.02298675663703747\\n16315,3.772459375054173e-05\\n69947,0.14425350562706088\\n230764,0.04512536052208172\\n253405,0.05804819636402895\\n204647,0.017058276856885032\\n67827,0.12091436111411591\\n48938,0.004728105316820242\\n35164,2.3480781038691894e-05\\n280421,0.5841989396088785\\n2697,8.53009649640435e-08\\n26576,0.0010560110060689346\\n81080,4.27935968274'\n"
     ]
    }
   ],
   "source": [
    "with open(\"./submission_test.csv\", \"rb\") as f:\n",
    "    submission = f.read()\n",
    "    print(submission[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 tpr: 0.052333333333333336, auc: 0.5071444444444444\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "result = requests.post(\n",
    "    \"http://149.156.182.9:6060/task-1/submit\",\n",
    "    headers={\"token\": \"i2SLZ1KbTzJeGkfPTWxE2Y53W9D0R5\"},\n",
    "    files={\n",
    "        \"csv_file\": (\"submission.csv\", open(\"./submission_test.csv\", \"rb\"))\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result.status_code, result.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
