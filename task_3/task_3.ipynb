{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage\n",
    "from tqdm import tqdm\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "\n",
    "        self.ids = []\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.imgs_tensors = []\n",
    "        self.labels_tensors = []\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def compute_transform(self):\n",
    "        self.imgs_tensors = []\n",
    "        self.labels_tensors = []\n",
    "\n",
    "        for img in self.imgs:\n",
    "            img = (transforms.functional.pil_to_tensor(img).float() / 255.0)\n",
    "            if len(img.shape) == 2:\n",
    "                img = torch.stack((img,)*3, axis=-1)\n",
    "            elif img.shape[0] == 1:\n",
    "                img = torch.cat((img,)*3, axis=0)\n",
    "            self.imgs_tensors.append(img)\n",
    "\n",
    "        for label in self.labels:\n",
    "            label_tensor = torch.tensor(label)\n",
    "            self.labels_tensors.append(label_tensor)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[int, torch.Tensor, int]:\n",
    "        id_ = self.ids[index]\n",
    "        img = self.imgs_tensors[index]\n",
    "        label = self.labels_tensors[index]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return id_, img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = v2.Compose([\n",
    "    v2.Normalize([0.2980, 0.2962, 0.2987], [0.2886, 0.2875, 0.2889]),\n",
    "    v2.Resize((32, 32))\n",
    "])\n",
    "\n",
    "train_transform = v2.Compose([\n",
    "    v2.Resize((32, 32)),\n",
    "\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomRotation(20),\n",
    "    v2.RandomAffine(0, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=0.05),\n",
    "\n",
    "    v2.RandomChoice([\n",
    "        v2.GaussianBlur(3),\n",
    "        v2.GaussianBlur(5),\n",
    "        lambda x: x\n",
    "    ], p=[0.1, 0.1, 0.8]),\n",
    "\n",
    "    v2.Normalize([0.2980, 0.2962, 0.2987], [0.2886, 0.2875, 0.2889]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load('Train.pt', weights_only=False)\n",
    "train_dataset.transform = train_transform\n",
    "train_dataset.compute_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = {}\n",
    "\n",
    "# for _, img, label in tqdm(train_dataset):\n",
    "#     label_value = int(label.item())\n",
    "#     if label_value not in labels:\n",
    "#         labels[label_value] = 1\n",
    "#     else:\n",
    "#         labels[label_value] += 1\n",
    "\n",
    "labels = {\n",
    "    8: 31802,\n",
    "    2: 23356,\n",
    "    5: 14886,\n",
    "    4: 7036,\n",
    "    1: 9171,\n",
    "    7: 409,\n",
    "    9: 4706,\n",
    "    3: 3568,\n",
    "    6: 4642,\n",
    "    0: 424\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: 31802,\n",
       " 2: 23356,\n",
       " 5: 14886,\n",
       " 4: 7036,\n",
       " 1: 9171,\n",
       " 7: 409,\n",
       " 9: 4706,\n",
       " 3: 3568,\n",
       " 6: 4642,\n",
       " 0: 424}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(labels.keys(), labels.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_features(img: np.ndarray) -> np.ndarray:\n",
    "#     # img_gray = img.mean(axis=2)\n",
    "#     img_sobel_x = skimage.filters.sobel_h(img)\n",
    "#     img_sobel_y = skimage.filters.sobel_v(img)\n",
    "#     img_sobel = np.sqrt(img_sobel_x ** 2 + img_sobel_y ** 2)\n",
    "#     return img_sobel\n",
    "\n",
    "# counter_colours = 0\n",
    "\n",
    "# for _, img, label in train_dataset:\n",
    "#     img_np = np.array(img)\n",
    "#     if len(img_np.shape) == 2:\n",
    "#         continue\n",
    "\n",
    "#     dif = img_np[:, :, 0] - img_np[:, :, 1]\n",
    "#     dif_2 = img_np[:, :, 1] - img_np[:, :, 2]\n",
    "#     # if np.any(dif != 0) or np.any(dif_2 != 0):\n",
    "#     #     # print('Found')\n",
    "#     #     # print(dif.sum())\n",
    "#     #     # print(dif_2.sum())\n",
    "#     #     plt.imshow(dif)\n",
    "#     #     plt.show()\n",
    "#     #     counter_colours += 1\n",
    "#     # print(img_np[: ,: ,0] - img_np[: ,: ,1], img_np[: ,: ,1] - img_np[: ,: ,2])\n",
    "\n",
    "#     dif_real = np.abs(dif) + np.abs(dif_2)\n",
    "#     is_object = dif_real > 1\n",
    "#     # img_sobel = get_features(dif_real)\n",
    "#     # img_sobel_sum = img_sobel.sum()\n",
    "\n",
    "    \n",
    "#     # if img_sobel_sum < 30:\n",
    "#     #     continue\n",
    "\n",
    "#     countours_found = skimage.measure.find_contours(is_object, 0.5)\n",
    "    \n",
    "#     if len(countours_found) == 0:\n",
    "#         continue\n",
    "#     print(countours_found)\n",
    "    \n",
    "#     plt.imshow(is_object)\n",
    "#     plt.show()\n",
    "\n",
    "#     # plt.title(f'{img_sobel_sum} {label}')\n",
    "#     # plt.imshow(img_sobel)\n",
    "#     # plt.show()\n",
    "#     plt.imshow(img_np)\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "# print(counter_colours)\n",
    "# print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(512, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([235.8491,  10.9039,   4.2816,  28.0269,  14.2126,   6.7177,  21.5424,\n",
      "        244.4988,   3.1445,  21.2495], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "total_count = sum(labels.values())\n",
    "class_weights = [total_count / labels[label] for label in sorted(labels.keys())]\n",
    "class_weights = torch.tensor(class_weights).to(device)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, optimizer: optim.Optimizer, criterion, train_dataloader: DataLoader,\n",
    "          epochs: int, epsilon: float = 0.01, alpha: float = 0.5):\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()\n",
    "        for id_, img, label in tqdm(train_dataloader):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # img.requires_grad = True\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img)\n",
    "\n",
    "            # lowest_class = outputs.argmin(axis=1)\n",
    "            # lowest_class_loss = criterion(outputs, lowest_class)\n",
    "            # lowest_class_loss.backward(retain_graph=True)\n",
    "            # new_img = img + epsilon * torch.sign(img.grad)\n",
    "\n",
    "            # outputs_new = model(new_img)\n",
    "            loss = alpha * criterion(outputs, label)# + (1 - alpha) * criterion(outputs_new, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (predicted == label).sum().item()\n",
    "\n",
    "            total += len(label)\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        accuracy = total_correct / total\n",
    "        print(f'Epoch: {epoch} Loss: {avg_loss} Accuracy: {accuracy}')\n",
    "        loss_history.append(avg_loss)\n",
    "        accuracy_history.append(accuracy)\n",
    "\n",
    "    return loss_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 87.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.004300143235325814 Accuracy: 0.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 89.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.0041301077684760094 Accuracy: 0.54093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 89.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.004136888377666474 Accuracy: 0.54362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 91.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.004096948882341385 Accuracy: 0.54698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 89.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 0.004080575917065144 Accuracy: 0.55013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:09<00:00, 86.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 0.004054425373673439 Accuracy: 0.55191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 87.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 0.004054087252914906 Accuracy: 0.54937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 89.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 0.004023235460817814 Accuracy: 0.55397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 90.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 0.0040217251273989675 Accuracy: 0.55822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 87.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 0.003984970298707485 Accuracy: 0.55597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 89.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 0.00397030341565609 Accuracy: 0.55986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 91.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 0.0039380763626098636 Accuracy: 0.56158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 86.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 0.003957518102973699 Accuracy: 0.56081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 90.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Loss: 0.003916550622582436 Accuracy: 0.56287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 0/782 [00:00<?, ?it/s]Exception ignored in: <function _releaseLock at 0x146e18d062a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/net/tscratch/people/tutorial004/ai/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n",
      "100%|███████████████████████████████████████████████████████████████████████| 782/782 [00:08<00:00, 89.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Loss: 0.003914352546930313 Accuracy: 0.56416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████████████████████▏                                   | 387/782 [00:04<00:05, 78.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.layer4.parameters():\n\u001b[32m      4\u001b[39m     param.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m loss_history_new, acc_history_new = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m loss_history += loss_history_new\n\u001b[32m      8\u001b[39m acc_history += acc_history_new\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, criterion, train_dataloader, epochs, epsilon, alpha)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# img.requires_grad = True\u001b[39;00m\n\u001b[32m     18\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# lowest_class = outputs.argmin(axis=1)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# lowest_class_loss = criterion(outputs, lowest_class)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# lowest_class_loss.backward(retain_graph=True)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# new_img = img + epsilon * torch.sign(img.grad)\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# outputs_new = model(new_img)\u001b[39;00m\n\u001b[32m     27\u001b[39m loss = alpha * criterion(outputs, label)\u001b[38;5;66;03m# + (1 - alpha) * criterion(outputs_new, label)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torchvision/models/resnet.py:274\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    271\u001b[39m x = \u001b[38;5;28mself\u001b[39m.maxpool(x)\n\u001b[32m    273\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer1(x)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer3(x)\n\u001b[32m    276\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer4(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torchvision/models/resnet.py:97\u001b[39m, in \u001b[36mBasicBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     94\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu(out)\n\u001b[32m     96\u001b[39m out = \u001b[38;5;28mself\u001b[39m.conv2(out)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    100\u001b[39m     identity = \u001b[38;5;28mself\u001b[39m.downsample(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/tscratch/people/tutorial004/ai/lib/python3.12/site-packages/torch/nn/functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss_history, acc_history = train(model, optimizer, criterion, train_dataloader, 2)\n",
    "\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "loss_history_new, acc_history_new = train(model, optimizer, criterion, train_dataloader, 20)\n",
    "loss_history += loss_history_new\n",
    "acc_history += acc_history_new\n",
    "\n",
    "for param in model.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "loss_history_new, acc_history_new = train(model, optimizer, criterion, train_dataloader, 30)\n",
    "loss_history += loss_history_new\n",
    "acc_history += acc_history_new\n",
    "\n",
    "for param in model.layer2.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "loss_history_new, acc_history_new = train(model, optimizer, criterion, train_dataloader, 40)\n",
    "loss_history += loss_history_new\n",
    "acc_history += acc_history_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
